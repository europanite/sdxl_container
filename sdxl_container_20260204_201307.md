<!-- P2M_REPORT -->
<!-- GENERATED at 2026-02-04 20:13:07 -->
# Project Export: sdxl_container

## Overview

- Root: `/home/skinner/sdxl_container`
- Files: **36**
- Total size: **57607 bytes**
- Total LOC: 1678 | SLOC: 1349 | TODOs: 0

### Language mix
- plain: 14
- yaml: 7
- markdown: 6
- bash: 3
- python: 2
- html: 1
- toml: 1
- ini: 1
- dockerfile: 1

### Top 12 largest files (bytes)
- `LICENSE` ‚Äî 11340 bytes
- `.gitignore` ‚Äî 4725 bytes
- `.github/workflows/codeql.yml` ‚Äî 4693 bytes
- `CODE_OF_CONDUCT.md` ‚Äî 4085 bytes
- `scripts/infer_sdxl_lora.py` ‚Äî 3872 bytes
- `scripts/make_lora.sh` ‚Äî 3547 bytes
- `scripts/train_network.sh` ‚Äî 3278 bytes
- `.github/workflows/pytest.yml` ‚Äî 2521 bytes
- `scripts/caption_images.py` ‚Äî 2496 bytes
- `.github/workflows/ci.yml` ‚Äî 2230 bytes
- `service/Dockerfile` ‚Äî 2057 bytes
- `service/Dockerfile.test` ‚Äî 2046 bytes

### Top 12 longest files (LOC)
- `.gitignore` ‚Äî 211 LOC
- `LICENSE` ‚Äî 201 LOC
- `scripts/make_lora.sh` ‚Äî 128 LOC
- `scripts/infer_sdxl_lora.py` ‚Äî 109 LOC
- `.github/workflows/codeql.yml` ‚Äî 100 LOC
- `scripts/train_network.sh` ‚Äî 94 LOC
- `README.md` ‚Äî 85 LOC
- `.github/workflows/ci.yml` ‚Äî 81 LOC
- `.github/workflows/pytest.yml` ‚Äî 78 LOC
- `scripts/caption_images.py` ‚Äî 70 LOC
- `CODE_OF_CONDUCT.md` ‚Äî 68 LOC
- `.dockerignore` ‚Äî 57 LOC

### Project tree (included subset)
```
sdxl_container/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îú‚îÄ‚îÄ ISSUE_TEMPLATE/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bug_report.md
‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ci.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ codeql.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lint.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pytest.yml
‚îÇ   ‚îî‚îÄ‚îÄ pull_request_template.md
‚îú‚îÄ‚îÄ .ruff_cache/
‚îÇ   ‚îú‚îÄ‚îÄ 0.14.13/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 14227307826790736989
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore
‚îÇ   ‚îî‚îÄ‚îÄ CACHEDIR.TAG
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ caption_images.py
‚îÇ   ‚îú‚îÄ‚îÄ entrypoint.sh
‚îÇ   ‚îú‚îÄ‚îÄ infer_sdxl_lora.py
‚îÇ   ‚îú‚îÄ‚îÄ make_lora.sh
‚îÇ   ‚îî‚îÄ‚îÄ train_network.sh
‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.test
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îú‚îÄ‚îÄ pytest.ini
‚îÇ   ‚îú‚îÄ‚îÄ requirements.test.txt
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ workspace/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ _config.yml
‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îú‚îÄ‚îÄ docker-compose.test.yml
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ google095bf08db4fb15d0.html
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ SECURITY.md
```

## Table of contents (files)

- 1. [.dockerignore](#.dockerignore)
- 2. [.env](#.env)
- 3. [.github/ISSUE_TEMPLATE/bug_report.md](#.github-ISSUE_TEMPLATE-bug_report.md)
- 4. [.github/pull_request_template.md](#.github-pull_request_template.md)
- 5. [.github/workflows/ci.yml](#.github-workflows-ci.yml)
- 6. [.github/workflows/codeql.yml](#.github-workflows-codeql.yml)
- 7. [.github/workflows/lint.yml](#.github-workflows-lint.yml)
- 8. [.github/workflows/pytest.yml](#.github-workflows-pytest.yml)
- 9. [.gitignore](#.gitignore)
- 10. [.ruff_cache/.gitignore](#.ruff_cache-.gitignore)
- 11. [.ruff_cache/0.14.13/14227307826790736989](#.ruff_cache-0.14.13-14227307826790736989)
- 12. [.ruff_cache/CACHEDIR.TAG](#.ruff_cache-CACHEDIR.TAG)
- 13. [_config.yml](#_config.yml)
- 14. [CODE_OF_CONDUCT.md](#CODE_OF_CONDUCT.md)
- 15. [CONTRIBUTING.md](#CONTRIBUTING.md)
- 16. [datasets/.gitkeep](#datasets-.gitkeep)
- 17. [docker-compose.test.yml](#docker-compose.test.yml)
- 18. [docker-compose.yml](#docker-compose.yml)
- 19. [google095bf08db4fb15d0.html](#google095bf08db4fb15d0.html)
- 20. [LICENSE](#LICENSE)
- 21. [models/.gitkeep](#models-.gitkeep)
- 22. [README.md](#README.md)
- 23. [scripts/caption_images.py](#scripts-caption_images.py)
- 24. [scripts/entrypoint.sh](#scripts-entrypoint.sh)
- 25. [scripts/infer_sdxl_lora.py](#scripts-infer_sdxl_lora.py)
- 26. [scripts/make_lora.sh](#scripts-make_lora.sh)
- 27. [scripts/train_network.sh](#scripts-train_network.sh)
- 28. [SECURITY.md](#SECURITY.md)
- 29. [service/Dockerfile](#service-Dockerfile)
- 30. [service/Dockerfile.test](#service-Dockerfile.test)
- 31. [service/pyproject.toml](#service-pyproject.toml)
- 32. [service/pytest.ini](#service-pytest.ini)
- 33. [service/requirements.test.txt](#service-requirements.test.txt)
- 34. [service/requirements.txt](#service-requirements.txt)
- 35. [tests/.gitkeep](#tests-.gitkeep)
- 36. [workspace/.gitkeep](#workspace-.gitkeep)

---

## Files

<a id=".dockerignore"></a>
### 1. `.dockerignore`
- Size: 612 bytes | LOC: 57 | SLOC: 48 | TODOs: 0 | Modified: 2026-02-03 11:51:03 | SHA1: 910043b0ee01

#### Brief
# Git
.git

#### Auto Summary
# Git

#### Content

```
# Git
.git
.gitignore
.gitattributes
**/.gitkeep

# Python cache
__pycache__/
*.pyc
*.pyo
*.pyd
*.pkl

# Virtual environments
.venv/
venv/
env/
ENV/

# Jupyter checkpoints
.ipynb_checkpoints/

# Logs / temp
*.log
*.tmp
*.swp

# OS / editor junk
.DS_Store
Thumbs.db
.idea/
.vscode/

# Docker specific
.dockerignore
docker-compose.override.yml

# Local config (don't bake secrets into images)
.env
.env.*
!env.example

# Build artifacts
*.egg-info/
.eggs/
dist/
build/

.git
**/__pycache__/
**/*.pyc
**/.ipynb_checkpoints/
.env
workspace/cache/
workspace/runs/
/models/base/**/*.ckpt
/models/base/**/*.safetensors
```

<a id=".env"></a>
### 2. `.env`
- Size: 40 bytes | LOC: 2 | SLOC: 2 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 19ae5840835c

#### Brief
HOST_USERNAME=user
HOST_GROUPNAME=group

#### Auto Summary
HOST_USERNAME=user

#### Content

```
HOST_USERNAME=user
HOST_GROUPNAME=group
```

<a id=".github-ISSUE_TEMPLATE-bug_report.md"></a>
### 3. `.github/ISSUE_TEMPLATE/bug_report.md`
- Size: 666 bytes | LOC: 30 | SLOC: 24 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 4b4d2b96744a

#### Brief
---
name: Bug Report

#### Auto Summary
Description

#### Content (verbatim)

```markdown
---
name: Bug Report
about: Create a report to help us improve
title: "[Bug] "
labels: bug
assignees: ''
---

## Description
<!-- A clear and concise description of what the bug is -->

## Steps to Reproduce
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

## Expected Behavior
<!-- A clear and concise description of what you expected to happen -->

## Screenshots
<!-- If applicable, add screenshots to help explain your problem -->

## Environment
- OS: [e.g. Ubuntu 22.04]
- Browser/Version: [e.g. Chrome 117]
- Node/Python Version: [e.g. Node 18, Python 3.10]

## Additional Context
<!-- Add any other context about the problem here -->
```

<a id=".github-pull_request_template.md"></a>
### 4. `.github/pull_request_template.md`
- Size: 744 bytes | LOC: 27 | SLOC: 21 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 1b6e1aab4d9a

#### Brief
# Pull Request

#### Auto Summary
Pull Request

#### Content (verbatim)

```markdown
# Pull Request

## Overview
<!-- Briefly describe the purpose of this PR and the problem it solves   -->

## Changes
<!-- List the main changes made in this PR -->
- 

## Testing
<!-- Describe how you tested the changes -->
- [ ] Built and ran locally without errors
- [ ] All tests passed
- [ ] Verified functionality manually (describe how)

## Related Issues
<!-- Link to related issues if applicable -->
- Closes #

## Checklist
- [ ] Code is clean and free of unnecessary comments/debug prints
- [ ] Proper naming conventions and documentation are followed
- [ ] Updated documentation/README if necessary
- [ ] CI pipeline passes successfully

## Notes for Reviewers
<!-- Additional context or things reviewers should pay attention to -->
```

<a id=".github-workflows-ci.yml"></a>
### 5. `.github/workflows/ci.yml`
- Size: 2230 bytes | LOC: 81 | SLOC: 81 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 7ce14f0fe806

#### Brief
name: CI
on:

#### Auto Summary
name: CI

#### Content

```yaml
name: CI
on:
  push:
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  build-test-images:
    runs-on: ${{ matrix.os }}
    permissions:
      contents: read
      actions: write
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest ]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build service TEST image 
        env:
          DOCKER_BUILDKIT: 1
        run: |
          docker build \
            -f service/Dockerfile.test \
            -t local/service-test:ci \
            service
      - name: Save images for downstream jobs
        run: |
          docker save local/service-test:ci | gzip > service-test.tar.gz
      - uses: actions/upload-artifact@v4
        with:
          name: test-images
          path: service-test.tar.gz
  service-tests:
    runs-on: ${{ matrix.os }}
    permissions:
      contents: read
      actions: write
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest ]
    needs: build-test-images
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
        with:
          name: test-images
      - name: Load images
        run: |
          gunzip -c service-test.tar.gz | docker load
      - name: Prepare report dir on host (mounted into /reports)
        run: mkdir -p service/reports/ci
      - name: Run service tests
        run: |
          docker compose -f docker-compose.test.yml run --rm   --entrypoint /bin/sh service_test -lc '
              python -m coverage run -m pytest
            '
      - name: Upload coverage xml
        uses: actions/upload-artifact@v4
        with:
          name: coverage-xml
          path: service/reports/ci/coverage.xml
          if-no-files-found: error
      - name: Cleanup
        env:
          DOCKER_BUILDKIT: 1
          HOST_UID: "1000"
          HOST_GID: "1000"
          DISPLAY: ":1"
          HOST_USERNAME: user
          HOST_GROUPNAME: group
        if: always()
        run: |
          docker compose  down -v --remove-orphans
```

<a id=".github-workflows-codeql.yml"></a>
### 6. `.github/workflows/codeql.yml`
- Size: 4693 bytes | LOC: 100 | SLOC: 46 | TODOs: 0 | Modified: 2026-02-03 11:48:50 | SHA1: 462909940e40

#### Brief
For most projects, this workflow file will not need changing; you simply need
to commit it to your repository.

You may wish to alter this file to override the set of languages analyzed,
or to provide custom queries or build logic.

#### Auto Summary
# For most projects, this workflow file will not need changing; you simply need

#### Content

```yaml
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL Advanced"

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  schedule:
    - cron: '0 0 * * *'

jobs:
  analyze:
    name: Analyze (${{ matrix.language }})
    # Runner size impacts CodeQL analysis time. To learn more, please see:
    #   - https://gh.io/recommended-hardware-resources-for-running-codeql
    #   - https://gh.io/supported-runners-and-hardware-resources
    #   - https://gh.io/using-larger-runners (GitHub.com only)
    # Consider using larger runners or machines with greater resources for possible analysis time improvements.
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    permissions:
      # required for all workflows
      security-events: write

      # required to fetch internal or private CodeQL packs
      packages: read

      # only required for workflows in private repositories
      actions: read
      contents: read

    strategy:
      fail-fast: false
      matrix:
        include:
        - language: actions
          build-mode: none
        - language: python
          build-mode: none
        # CodeQL supports the following values keywords for 'language': 'actions', 'c-cpp', 'csharp', 'go', 'java-kotlin', 'javascript-typescript', 'python', 'ruby', 'rust', 'swift'
        # Use `c-cpp` to analyze code written in C, C++ or both
        # Use 'java-kotlin' to analyze code written in Java, Kotlin or both
        # Use 'javascript-typescript' to analyze code written in JavaScript, TypeScript or both
        # To learn more about changing the languages that are analyzed or customizing the build mode for your analysis,
        # see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/customizing-your-advanced-setup-for-code-scanning.
        # If you are analyzing a compiled language, you can modify the 'build-mode' for that language to customize how
        # your codebase is analyzed, see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/codeql-code-scanning-for-compiled-languages
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # Add any setup steps before running the `github/codeql-action/init` action.
    # This includes steps like installing compilers or runtimes (`actions/setup-node`
    # or others). This is typically only required for manual builds.
    # - name: Setup runtime (example)
    #   uses: actions/setup-example@v1

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v4
      with:
        languages: ${{ matrix.language }}
        build-mode: ${{ matrix.build-mode }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality

    # If the analyze step fails for one of the languages you are analyzing with
    # "We were unable to automatically build your code", modify the matrix above
    # to set the build mode to "manual" for that language. Then modify this step
    # to build your code.
    # ‚ÑπÔ∏è Command-line programs to run using the OS shell.
    # üìö See https://docs.github.com/en/actions/using-workflows/workspaceflow-syntax-for-github-actions#jobsjob_idstepsrun
    - if: matrix.build-mode == 'manual'
      shell: bash
      run: |
        echo 'If you are using a "manual" build mode for one or more of the' \
          'languages you are analyzing, replace this with the commands to build' \
          'your code, for example:'
        echo '  make bootstrap'
        echo '  make release'
        exit 1

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v4
      with:
        category: "/language:${{matrix.language}}"
```

<a id=".github-workflows-lint.yml"></a>
### 7. `.github/workflows/lint.yml`
- Size: 1056 bytes | LOC: 38 | SLOC: 38 | TODOs: 0 | Modified: 2026-02-04 19:46:12 | SHA1: 25b271909a50

#### Brief
name: Python Lint
on:

#### Auto Summary
name: Python Lint

#### Content

```yaml
name: Python Lint
on:
  push:
  workflow_dispatch:
  pull_request:
  schedule:
    - cron: '0 0 * * *'
permissions:
  security-events: write
jobs:
  lint:
    name: Python ${{ matrix.python-version }} ‚Ä¢ ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, windows-latest ]
        python-version: [ "3.10", "3.11", "3.12", "3.13" ]
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"
      - name: Install tools
        run: |
          python -m pip install -U pip
          pip install -r service/requirements.test.txt
      - name: Ruff (lint)
        run: ruff check .
      - name: Ruff SARIF
        if: always()
        run: ruff check --output-format sarif . > ruff.sarif || true
      - name: Upload SARIF to Code Scanning
        uses: github/codeql-action/upload-sarif@v4
        with:
          sarif_file: ruff.sarif
```

<a id=".github-workflows-pytest.yml"></a>
### 8. `.github/workflows/pytest.yml`
- Size: 2521 bytes | LOC: 78 | SLOC: 76 | TODOs: 0 | Modified: 2026-02-04 19:46:18 | SHA1: 1e4911d18bea

#### Brief
name: Pytest
on:

#### Auto Summary
name: Pytest

#### Content

```yaml
name: Pytest
on:
  push:
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  tests:
    name: pytest - ${{ matrix.os }} - py${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    permissions:
      contents: read
      actions: write
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, windows-latest ]
        python-version: [ "3.10", "3.11", "3.12", "3.13" ]
    env:
      PYTHONUTF8: "1"
      PYTHONPATH: ${{ github.workspace }}/service
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install deps (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        shell: bash
        run: |
          set -eux
          python -m pip install -U pip
          pip install -r service/requirements.test.txt
          if [ -f service/pyproject.toml ] || [ -f service/setup.cfg ] || [ -f service/setup.py ]; then
            pip install -e service
          fi
      - name: Install deps (Windows)
        if: matrix.os == 'windows-latest'
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"
          python -m pip install -U pip
          pip install -r service/requirements.test.txt
          $hasPythonProject = (Test-Path -Path 'service/pyproject.toml') -or
                            (Test-Path -Path 'service/setup.cfg') -or
                            (Test-Path -Path 'service/setup.py')
          if ($hasPythonProject) {
            python -m pip install -e service
          } else {
            Write-Host 'service packaging files not found; skipping'
          }
      - name: Show versions
        shell: bash
        run: |
          python --version
          pip --version
          pytest --version
          python -c "import sys,platform,os;print(platform.platform());print(sys.version);print('PYTHONPATH=',os.environ.get('PYTHONPATH'))"
      - name: Run tests with coverage
        shell: bash
        run: |
          python -m coverage run -m pytest -q
          python -m coverage xml -o coverage.xml
      - name: Upload coverage.xml
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.os }}-py${{ matrix.python-version }}
          path: coverage.xml
          if-no-files-found: error
```

<a id=".gitignore"></a>
### 9. `.gitignore`
- Size: 4725 bytes | LOC: 211 | SLOC: 173 | TODOs: 0 | Modified: 2026-02-04 18:54:36 | SHA1: b6f8d30c69ac

#### Brief
# Byte-compiled / optimized / DLL files
__pycache__/

#### Auto Summary
# Byte-compiled / optimized / DLL files

#### Content

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

# workspace
workspace/
models/
datasetts/
```

<a id=".ruff_cache-.gitignore"></a>
### 10. `.ruff_cache/.gitignore`
- Size: 35 bytes | LOC: 2 | SLOC: 2 | TODOs: 0 | Modified: 2026-02-04 19:29:11 | SHA1: aeb40107cf37

#### Brief
# Automatically created by ruff.
*

#### Auto Summary
# Automatically created by ruff.

#### Content

```
# Automatically created by ruff.
*
```

<a id=".ruff_cache-0.14.13-14227307826790736989"></a>
### 11. `.ruff_cache/0.14.13/14227307826790736989`
- Size: 115 bytes | LOC: 0 | SLOC: 0 | TODOs: 0 | Modified: 2026-02-04 19:29:13 | SHA1: 
#### Content

```
```

<a id=".ruff_cache-CACHEDIR.TAG"></a>
### 12. `.ruff_cache/CACHEDIR.TAG`
- Size: 43 bytes | LOC: 1 | SLOC: 1 | TODOs: 0 | Modified: 2026-02-04 19:29:11 | SHA1: a841ba867b07

#### Brief
Signature: 8a477f597d28d172789f06886806bc55

#### Auto Summary
Signature: 8a477f597d28d172789f06886806bc55

#### Content

```
Signature: 8a477f597d28d172789f06886806bc55
```

<a id="_config.yml"></a>
### 13. `_config.yml`
- Size: 243 bytes | LOC: 10 | SLOC: 10 | TODOs: 0 | Modified: 2026-02-04 19:53:34 | SHA1: 716f4476fa0a

#### Brief
title: "sdxl_container"
description: "Python environment built with Docker Compose"

#### Auto Summary
title: "sdxl_container"

#### Content

```yaml
title: "sdxl_container"
description: "Python environment built with Docker Compose"
baseurl: "/sdxl_container"
url: "https://europanite.github.io"
theme: minima
markdown: kramdown
plugins:
  - jekyll-feed
  - jekyll-sitemap
highlighter: rouge
```

<a id="CODE_OF_CONDUCT.md"></a>
### 14. `CODE_OF_CONDUCT.md`
- Size: 4085 bytes | LOC: 68 | SLOC: 50 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 724cc33f0dca

#### Brief
# Contributor Covenant Code of Conduct

#### Auto Summary
Contributor Covenant Code of Conduct

#### Content (verbatim)

```markdown
# Contributor Covenant Code of Conduct

## Our Pledge
We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.

## Our Standards
Examples of behavior that contributes to a positive environment include:
- Demonstrating empathy and kindness toward other people
- Being respectful of differing opinions, viewpoints, and experiences
- Giving and gracefully accepting constructive feedback
- Taking responsibility and apologizing to those affected by our mistakes
- Focusing on what is best for the community, not just for ourselves

Examples of unacceptable behavior include:
- The use of sexualized language or imagery, and sexual attention or advances
- Trolling, insulting or derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others‚Äô private information without explicit permission
- Other conduct which could reasonably be considered inappropriate in a professional setting

## Enforcement Responsibilities
Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

## Scope
This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces.

## Enforcement
Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the maintainers of this project. All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the reporter of any incident.

## Enforcement Guidelines
Community leaders will follow these guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:

1. **Correction**  
- *Impact*: Use of inappropriate language or other behavior deemed unprofessional.  
- *Consequence*: A private, written warning, with clarity around the nature of the violation and an explanation of why the behavior was inappropriate.

2. **Warning**  
- *Impact*: A violation through a single incident or series of actions.  
- *Consequence*: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time.

3. **Temporary Ban**  
- *Impact*: A serious violation of community standards, including sustained inappropriate behavior.  
- *Consequence*: A temporary ban from any sort of interaction or public communication with the community for a specified period of time.

4. **Permanent Ban**  
- *Impact*: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.  
- *Consequence*: A permanent ban from any sort of public interaction within the
   community.

## Attribution

This Code of Conduct is adapted from the  
- [Contributor Covenant][v2.1], version 2.1, available at [https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by  
- [Mozilla‚Äôs code of conduct enforcement ladder][mozilla-coc].

For answers to common questions about this code of conduct, see the FAQ at  
- [Contributor Covenant FAQ][faq].

[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[mozilla-coc]: https://github.com/mozilla/diversity
[faq]: https://www.contributor-covenant.org/faq
```

<a id="CONTRIBUTING.md"></a>
### 15. `CONTRIBUTING.md`
- Size: 834 bytes | LOC: 25 | SLOC: 19 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 12ea21d94090

#### Brief
# Contributing Guidelines

#### Auto Summary
Contributing Guidelines

#### Content (verbatim)

```markdown
# Contributing Guidelines

Thank you for considering contributing to this project!  
We welcome bug reports, feature requests, and pull requests.  
Please follow the guidelines below to make the process smooth for everyone.

---

## How to Contribute

### 1. Reporting Issues
- Check the [issue tracker](../../issues) to avoid duplicates.
- Use the appropriate [issue template](.github/ISSUE_TEMPLATE/) when creating a new issue.
- Provide as much detail as possible (steps to reproduce, expected behavior, environment, etc.).

### 2. Suggesting Features
- Open a new **Feature Request** issue.
- Explain the problem your idea solves.
- Provide examples, use cases, or references if possible.

### 3. Submitting Pull Requests
Fork the repository and create a new branch:
   ```bash
   git checkout -b feature/your-feature-name
   ```
```

<a id="datasets-.gitkeep"></a>
### 16. `datasets/.gitkeep`
- Size: 0 bytes | LOC: 0 | SLOC: 0 | TODOs: 0 | Modified: 2026-02-04 19:00:51 | SHA1: 
#### Content

```
```

<a id="docker-compose.test.yml"></a>
### 17. `docker-compose.test.yml`
- Size: 955 bytes | LOC: 26 | SLOC: 24 | TODOs: 0 | Modified: 2026-02-04 19:26:40 | SHA1: bd026fa75506

#### Brief
services:
  trainer:

#### Auto Summary
services:

#### Content

```yaml
services:
  trainer:
    build:
      context: .
      dockerfile: service/Dockerfile
      args:
        SD_SCRIPTS_REPO: ${SD_SCRIPTS_REPO:-https://github.com/kohya-ss/sd-scripts.git}
        SD_SCRIPTS_REF: ${SD_SCRIPTS_REF:-main}
        KOHYA_REPO: ${KOHYA_REPO:-https://github.com/kohya-ss/kohya_ss.git}
        KOHYA_REF: ${KOHYA_REF:-master}
        TORCH_INDEX_URL: ${TORCH_INDEX_URL:-https://download.pytorch.org/whl/cu121}
    # Use bash to run the entrypoint so it works even if the file is not marked executable on the host.
    entrypoint: ["/bin/bash", "/scripts/entrypoint.sh"]
    volumes:
      - ./workspace:/workspace
      - ./scripts:/scripts
      - ./models:/models
      - ./datasets:/datasets
      - ./tests:/tests
    environment:
      HF_HOME: /workspace/cache/hf
      PIP_CACHE_DIR: /workspace/cache/pip
      PYTHONUNBUFFERED: "1"
    # If you have NVIDIA GPU + nvidia-container-toolkit:
    gpus: all
    shm_size: "8gb"
```

<a id="docker-compose.yml"></a>
### 18. `docker-compose.yml`
- Size: 932 bytes | LOC: 25 | SLOC: 23 | TODOs: 0 | Modified: 2026-02-03 11:50:08 | SHA1: 30ab40c2a033

#### Brief
services:
  trainer:

#### Auto Summary
services:

#### Content

```yaml
services:
  trainer:
    build:
      context: .
      dockerfile: service/Dockerfile
      args:
        SD_SCRIPTS_REPO: ${SD_SCRIPTS_REPO:-https://github.com/kohya-ss/sd-scripts.git}
        SD_SCRIPTS_REF: ${SD_SCRIPTS_REF:-main}
        KOHYA_REPO: ${KOHYA_REPO:-https://github.com/kohya-ss/kohya_ss.git}
        KOHYA_REF: ${KOHYA_REF:-master}
        TORCH_INDEX_URL: ${TORCH_INDEX_URL:-https://download.pytorch.org/whl/cu121}
    # Use bash to run the entrypoint so it works even if the file is not marked executable on the host.
    entrypoint: ["/bin/bash", "/scripts/entrypoint.sh"]
    volumes:
      - ./workspace:/workspace
      - ./scripts:/scripts
      - ./models:/models
      - ./datasets:/datasets
    environment:
      HF_HOME: /workspace/cache/hf
      PIP_CACHE_DIR: /workspace/cache/pip
      PYTHONUNBUFFERED: "1"
    # If you have NVIDIA GPU + nvidia-container-toolkit:
    gpus: all
    shm_size: "8gb"
```

<a id="google095bf08db4fb15d0.html"></a>
### 19. `google095bf08db4fb15d0.html`
- Size: 54 bytes | LOC: 1 | SLOC: 1 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: fd61b02d565d

#### Brief
google-site-verification: google095bf08db4fb15d0.html

#### Auto Summary
google-site-verification: google095bf08db4fb15d0.html

#### Content

```html
google-site-verification: google095bf08db4fb15d0.html
```

<a id="LICENSE"></a>
### 20. `LICENSE`
- Size: 11340 bytes | LOC: 201 | SLOC: 169 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 64f2208f2896

#### Brief
Apache License
                           Version 2.0, January 2004

#### Auto Summary
Apache License

#### Content

```
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2025 europanite

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
```

<a id="models-.gitkeep"></a>
### 21. `models/.gitkeep`
- Size: 0 bytes | LOC: 0 | SLOC: 0 | TODOs: 0 | Modified: 2026-02-04 19:00:51 | SHA1: 
#### Content

```
```

<a id="README.md"></a>
### 22. `README.md`
- Size: 1955 bytes | LOC: 85 | SLOC: 67 | TODOs: 0 | Modified: 2026-02-04 19:52:15 | SHA1: 02dd845c8efc

#### Brief
# [SDXL Container](https://github.com/europanite/sdxl_container "SDXL Container")

#### Auto Summary
[SDXL Container](https://github.com/europanite/sdxl_container "SDXL Container")

#### Content (verbatim)

```markdown
# [SDXL Container](https://github.com/europanite/sdxl_container "SDXL Container")

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/python-3.10%20|%203.11|%203.12|%203.13-blue)](https://www.python.org/)
![OS](https://img.shields.io/badge/OS-Linux%20%7C%20macOS%20%7C%20Windows-blue)

A docker container for SDXL

Highlights:
- **Reproducible**: everything runs inside a container (no local Python env needed).
- **Simple**: one command to (optionally) caption images + train.
- **Safe defaults** for few-shot SDXL LoRA.
- **Includes inference**: SDXL txt2img with LoRA using `diffusers`.

---

## Requirements

- Docker + Docker Compose v2
- A GPU-enabled Docker runtime is strongly recommended for training.

---

## Build

```bash
docker compose build trainer
```
---

## Train (caption + LoRA)

```bash
# train
docker compose run --rm trainer train \
--base-model /models/base/sd_xl_base_1.0.safetensors \
--images /datasets/subject/images \
--run-name title \
--sdxl \
--caption-mode blip \
--concept-token sksSubject \
--max-train-steps 1600 \
--num-repeats 20 \
--network-dim 16 \
--network-alpha 8
```

## Caption only (BLIP)

If you want to generate `.txt` captions next to each image (same basename):

```bash
# ## Caption only (BLIP)
docker compose run 
--rm trainer caption   
--images /datasets/subject/images   
--prefix sksSubject   
--overwrite
```

## Inference (SDXL txt2img with LoRA)

Generate images with the trained LoRA:

```bash
# inference
docker compose run 
--rm trainer infer   
--base-model /models/base/sd_xl_base_1.0.safetensors   
--lora /models/loras/subject_***.safetensors   
--prompt "sksSubject portrait photo"   
--negative-prompt ""   
--out-dir /work/outputs   
--num-images 4   
--steps 30   
--cfg 7.0   
--width 1024   
--height 1024   
--lora-scale 0.8   
--seed 42
```
---

## License
- Apache License 2.0
```

<a id="scripts-caption_images.py"></a>
### 23. `scripts/caption_images.py`
- Size: 2496 bytes | LOC: 70 | SLOC: 50 | TODOs: 0 | Modified: 2026-02-04 19:29:12 | SHA1: ed35eed8439b | Py: funcs=3                       classes=0                       complexity‚âà15

#### Brief
import argparse
from pathlib import Path

#### Auto Summary
Python module with 3 functions and 0 classes.

#### Content

```python
import argparse
from pathlib import Path

from PIL import Image


def try_load_blip():
    """
    Lazy import so the container can still run training even if caption deps are missing.
    """
    try:
        from transformers import BlipForConditionalGeneration, BlipProcessor  # type: ignore

        return BlipProcessor, BlipForConditionalGeneration
    except Exception as e:  # noqa: BLE001
        raise RuntimeError(
            "BLIP captioning dependencies are missing or failed to import. "
            "Install transformers + sentencepiece, and ensure torch is available.\n"
            f"Original error: {e}"
        ) from e


def iter_images(images_dir: Path):
    exts = {".png", ".jpg", ".jpeg", ".webp", ".bmp"}
    for p in sorted(images_dir.rglob("*")):
        if p.is_file() and p.suffix.lower() in exts:
            yield p


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--images", required=True, help="Directory containing training images")
    ap.add_argument("--prefix", default="", help="Prefix token (e.g., 'sksSubject') added to captions")
    ap.add_argument("--overwrite", action="store_true", help="Overwrite existing .txt captions")
    ap.add_argument("--max-new-tokens", type=int, default=40)
    args = ap.parse_args()

    images_dir = Path(args.images)
    if not images_dir.is_dir():
        raise SystemExit(f"Not a directory: {images_dir}")

    BlipProcessor, BlipForConditionalGeneration = try_load_blip()

    # Small caption model (base) to keep this lightweight
    model_id = "Salesforce/blip-image-captioning-base"
    processor = BlipProcessor.from_pretrained(model_id)
    model = BlipForConditionalGeneration.from_pretrained(model_id)

    prefix = (args.prefix.strip() + " ") if args.prefix.strip() else ""

    for img_path in iter_images(images_dir):
        cap_path = img_path.with_suffix(".txt")
        if cap_path.exists() and not args.overwrite:
            continue

        image = Image.open(img_path).convert("RGB")
        inputs = processor(image, return_tensors="pt")
        out = model.generate(**inputs, max_new_tokens=args.max_new_tokens)
        caption = processor.decode(out[0], skip_special_tokens=True).strip()

        # Very simple formatting: "<prefix><caption>"
        text = f"{prefix}{caption}\n"
        cap_path.write_text(text, encoding="utf-8")
        print(f"[captioned] {img_path.name} -> {cap_path.name}")

    print("Captioning finished.")


if __name__ == "__main__":
    main()
```

<a id="scripts-entrypoint.sh"></a>
### 24. `scripts/entrypoint.sh`
- Size: 398 bytes | LOC: 24 | SLOC: 20 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: f79eef56fa40

#### Brief
!/usr/bin/env bash

#### Auto Summary
#!/usr/bin/env bash

#### Content

```bash
#!/usr/bin/env bash
set -euo pipefail

cmd="${1:-bash}"
shift || true

case "$cmd" in
  train)
    exec bash /scripts/make_lora.sh "$@"
    ;;
  infer)
    exec python3 /scripts/infer_sdxl_lora.py "$@"
    ;;
  caption)
    exec python3 /scripts/caption_images.py "$@"
    ;;
  bash|sh)
    exec bash
    ;;
  *)
    # Allow arbitrary commands (e.g., "python3 -V")
    exec "$cmd" "$@"
    ;;
esac
```

<a id="scripts-infer_sdxl_lora.py"></a>
### 25. `scripts/infer_sdxl_lora.py`
- Size: 3872 bytes | LOC: 109 | SLOC: 81 | TODOs: 0 | Modified: 2026-02-03 11:48:50 | SHA1: 5bade62f41c8 | Py: funcs=4                       classes=0                       complexity‚âà20

#### Brief
!/usr/bin/env python3

#### Auto Summary
Python module with 4 functions and 0 classes.

#### Content

```python
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import random
import time
from pathlib import Path

import torch


def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="SDXL txt2img inference with LoRA (diffusers).")
    p.add_argument("--base-model", required=True, help="Base SDXL: dir or .safetensors")
    p.add_argument("--lora", required=True, help="LoRA weights: .safetensors")
    p.add_argument("--prompt", required=True)
    p.add_argument("--negative-prompt", default="")
    p.add_argument("--out-dir", default="/workspace/outputs")
    p.add_argument("--num-images", type=int, default=1)
    p.add_argument("--seed", type=int, default=-1, help="-1=random")
    p.add_argument("--steps", type=int, default=30)
    p.add_argument("--cfg", type=float, default=7.0)
    p.add_argument("--width", type=int, default=1024)
    p.add_argument("--height", type=int, default=1024)
    p.add_argument("--lora-scale", type=float, default=0.8)
    p.add_argument("--device", default="", help='e.g. "cuda", "cuda:0", "cpu" (default: auto)')
    p.add_argument("--cpu-offload", action="store_true")
    p.add_argument("--attention-slicing", action="store_true")
    return p


def load_pipe(base_model: str, device: str, dtype: torch.dtype):
    from diffusers import StableDiffusionXLPipeline  # type: ignore

    bm = Path(base_model)
    if bm.is_dir():
        pipe = StableDiffusionXLPipeline.from_pretrained(str(bm), torch_dtype=dtype, use_safetensors=True)
    else:
        if not hasattr(StableDiffusionXLPipeline, "from_single_file"):
            raise SystemExit("diffusers does not support from_single_file. Pass a diffusers-format directory to --base-model.")
        pipe = StableDiffusionXLPipeline.from_single_file(str(bm), torch_dtype=dtype)

    if device.startswith("cuda"):
        pipe.to(device)
    else:
        pipe.to("cpu")
    return pipe


def load_lora(pipe, lora_path: str):
    lp = Path(lora_path)
    # For compatibility, load using the "parent directory + weight_name" style.
    pipe.load_lora_weights(str(lp.parent), weight_name=lp.name)


def main() -> None:
    args = build_argparser().parse_args()

    device = args.device.strip() or ("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if device.startswith("cuda") else torch.float32

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    pipe = load_pipe(args.base_model, device, dtype)
    load_lora(pipe, args.lora)

    if args.attention_slicing and hasattr(pipe, "enable_attention_slicing"):
        try:
            pipe.enable_attention_slicing()
        except Exception:
            pass

    if args.cpu_offload and hasattr(pipe, "enable_model_cpu_offload"):
        pipe.enable_model_cpu_offload()

    ts = time.strftime("%Y%m%d_%H%M%S")

    for i in range(args.num_images):
        seed = args.seed
        if seed < 0:
            seed = random.randint(0, 2**31 - 1)
        else:
            seed = seed + i

        gen_device = device if device.startswith("cuda") else "cpu"
        generator = torch.Generator(device=gen_device).manual_seed(seed)

        # Apply LoRA scale via cross_attention_kwargs (to absorb diffusers version differences).
        extra = {"cross_attention_kwargs": {"scale": float(args.lora_scale)}}

        images = pipe(
            prompt=args.prompt,
            negative_prompt=args.negative_prompt or None,
            width=int(args.width),
            height=int(args.height),
            num_inference_steps=int(args.steps),
            guidance_scale=float(args.cfg),
            generator=generator,
            **extra,
        ).images

        out_path = out_dir / f"sdxl_lora_{ts}_{i:02d}_seed{seed}.png"
        images[0].save(out_path)
        print(out_path)


if __name__ == "__main__":
    main()
```

<a id="scripts-make_lora.sh"></a>
### 26. `scripts/make_lora.sh`
- Size: 3547 bytes | LOC: 128 | SLOC: 107 | TODOs: 0 | Modified: 2026-02-03 11:50:37 | SHA1: 20ae458f30e7

#### Brief
!/usr/bin/env bash

#### Auto Summary
#!/usr/bin/env bash

#### Content

```bash
#!/usr/bin/env bash
set -euo pipefail

usage() {
  cat <<'USAGE'
make_lora.sh - Train a LoRA from a small set of images.

Required:
  --base-model  /models/base/<model>.safetensors
  --images      /datasets/<subject>/images
  --run-name    e.g. my_subject

Optional (common):
  --concept-token   e.g. sksSubject
  --out-dir         default: /workspace/models/loras
  --caption-mode    none|blip (default: none)
  --sdxl            set SDXL-friendly defaults (1024px)
  --max-train-steps default: 1600
  --num-repeats     default: 20
  --network-dim     default: 16
  --network-alpha   default: 8
USAGE
}

BASE_MODEL=""
IMAGES_DIR=""
RUN_NAME=""
CONCEPT_TOKEN="sksSubject"
OUT_DIR="/workspace/models/loras"
CAPTION_MODE="none"
SDXL="0"
MAX_TRAIN_STEPS="1600"
NUM_REPEATS="20"
NETWORK_DIM="16"
NETWORK_ALPHA="8"

while [[ $# -gt 0 ]]; do
  case "$1" in
    --base-model) BASE_MODEL="$2"; shift 2;;
    --images) IMAGES_DIR="$2"; shift 2;;
    --run-name) RUN_NAME="$2"; shift 2;;
    --concept-token) CONCEPT_TOKEN="$2"; shift 2;;
    --out-dir) OUT_DIR="$2"; shift 2;;
    --caption-mode) CAPTION_MODE="$2"; shift 2;;
    --sdxl) SDXL="1"; shift 1;;
    --max-train-steps) MAX_TRAIN_STEPS="$2"; shift 2;;
    --num-repeats) NUM_REPEATS="$2"; shift 2;;
    --network-dim) NETWORK_DIM="$2"; shift 2;;
    --network-alpha) NETWORK_ALPHA="$2"; shift 2;;
    -h|--help) usage; exit 0;;
    *) echo "Unknown arg: $1"; usage; exit 1;;
  esac
done

if [[ -z "$BASE_MODEL" || -z "$IMAGES_DIR" || -z "$RUN_NAME" ]]; then
  echo "Missing required arguments."
  usage
  exit 1
fi

if [[ ! -f "$BASE_MODEL" ]]; then
  echo "Base model not found: $BASE_MODEL"
  exit 1
fi

if [[ ! -d "$IMAGES_DIR" ]]; then
  echo "Images dir not found: $IMAGES_DIR"
  exit 1
fi

TS="$(date +%Y%m%d_%H%M%S)"
RUN_DIR="/workspace/runs/${RUN_NAME}_${TS}"
DATA_DIR="${RUN_DIR}/dataset"
# sd-scripts "DreamBooth method" expects: <train_data_dir>/<repeats>_<token>/*.(png|jpg|webp) and optional captions
INSTANCE_DIR="${DATA_DIR}/${NUM_REPEATS}_${CONCEPT_TOKEN}"
mkdir -p "$RUN_DIR" "$DATA_DIR" "$INSTANCE_DIR" "$OUT_DIR" "/workspace/cache"

echo "Run dir: $RUN_DIR"
echo "Copying images into: $INSTANCE_DIR"
cp -a "${IMAGES_DIR}/." "$INSTANCE_DIR/"

# Sanity check: make sure we actually have images to train on
IMG_COUNT="$(find "$INSTANCE_DIR" -maxdepth 1 -type f \( \
  -iname "*.png" -o -iname "*.jpg" -o -iname "*.jpeg" -o -iname "*.webp" \) | wc -l)"
if [[ "$IMG_COUNT" -eq 0 ]]; then
  echo "ERROR: No images found under: $INSTANCE_DIR" >&2
  echo "       Check --images path and volume mounts." >&2
  exit 2
fi
echo "Found $IMG_COUNT training images."

# Captioning (optional): writes one .txt per image (same basename)
if [[ "$CAPTION_MODE" == "blip" ]]; then
  echo "Captioning with BLIP..."
  python3 /scripts/caption_images.py \
    --images "$INSTANCE_DIR" \
    --prefix "${CONCEPT_TOKEN}" \
    --overwrite
else
  echo "Captioning disabled (mode: ${CAPTION_MODE})."
fi

# Training defaults
RESOLUTION="768"
if [[ "$SDXL" == "1" ]]; then
  RESOLUTION="1024"
fi

echo "Starting training..."
TRAIN_ARGS=(
  --base-model "$BASE_MODEL"
  --train-data-dir "${DATA_DIR}"
  --output-dir "$OUT_DIR"
  --output-name "${RUN_NAME}_${TS}"
  --resolution "$RESOLUTION"
  --max-train-steps "$MAX_TRAIN_STEPS"
  --num-repeats "$NUM_REPEATS"
  --network-dim "$NETWORK_DIM"
  --network-alpha "$NETWORK_ALPHA"
)
if [[ "$SDXL" == "1" ]]; then
  TRAIN_ARGS+=(--sdxl)
fi

bash /scripts/train_network.sh "${TRAIN_ARGS[@]}"

echo "Done."
echo "LoRA saved under: ${OUT_DIR}/${RUN_NAME}_${TS}*"
```

<a id="scripts-train_network.sh"></a>
### 27. `scripts/train_network.sh`
- Size: 3278 bytes | LOC: 94 | SLOC: 75 | TODOs: 0 | Modified: 2026-02-03 02:17:02 | SHA1: 925b70b0335d

#### Brief
!/usr/bin/env bash

#### Auto Summary
#!/usr/bin/env bash

#### Content

```bash
#!/usr/bin/env bash
set -euo pipefail

# Wrapper around sd-scripts train_network.py (installed at /opt/sd-scripts)
# Adjust arguments as needed. This is a sane baseline for "few images" LoRA.

BASE_MODEL=""
TRAIN_DATA_DIR=""
OUTPUT_DIR=""
OUTPUT_NAME=""
RESOLUTION="768"
SDXL="0"
MAX_TRAIN_STEPS="1600"
NUM_REPEATS="20"
NETWORK_DIM="16"
NETWORK_ALPHA="8"

while [[ $# -gt 0 ]]; do
  case "$1" in
    --base-model) BASE_MODEL="$2"; shift 2;;
    --train-data-dir) TRAIN_DATA_DIR="$2"; shift 2;;
    --output-dir) OUTPUT_DIR="$2"; shift 2;;
    --output-name) OUTPUT_NAME="$2"; shift 2;;
    --resolution) RESOLUTION="$2"; shift 2;;
    --max-train-steps) MAX_TRAIN_STEPS="$2"; shift 2;;
    --num-repeats) NUM_REPEATS="$2"; shift 2;;
    --network-dim) NETWORK_DIM="$2"; shift 2;;
    --network-alpha) NETWORK_ALPHA="$2"; shift 2;;
    --sdxl) SDXL="1"; shift 1;;
    *) echo "Unknown arg: $1"; exit 1;;
  esac
done

if [[ -z "$BASE_MODEL" || -z "$TRAIN_DATA_DIR" || -z "$OUTPUT_DIR" || -z "$OUTPUT_NAME" ]]; then
  echo "train_network.sh: missing required args"
  exit 1
fi

mkdir -p "$OUTPUT_DIR"

# Reduce CUDA allocator fragmentation (recommended by PyTorch when OOM happens with small allocations)
export PYTORCH_ALLOC_CONF="${PYTORCH_ALLOC_CONF:-expandable_segments:True}"

# Select the correct training entrypoint.
# sd-scripts trains SDXL LoRA with sdxl_train_network.py (not train_network.py + --sdxl).
TRAIN_SCRIPT="/opt/sd-scripts/train_network.py"
CLIP_SKIP_ARG="--clip_skip=1"
if [[ "$SDXL" == "1" ]]; then
  TRAIN_SCRIPT="/opt/sd-scripts/sdxl_train_network.py"
  CLIP_SKIP_ARG=""
  if [[ ! -f "$TRAIN_SCRIPT" ]]; then
    echo "ERROR: SDXL mode was requested (--sdxl), but $TRAIN_SCRIPT was not found." >&2
    exit 2
  fi
fi

# Add memory-saving flags only when supported by the installed sd-scripts
HELP_TEXT="$(python3 "$TRAIN_SCRIPT" --help 2>&1 || true)"
EXTRA_ARGS=()
grep -q -- '--network_train_unet_only' <<<"$HELP_TEXT" && EXTRA_ARGS+=(--network_train_unet_only)
grep -q -- '--gradient_checkpointing' <<<"$HELP_TEXT" && EXTRA_ARGS+=(--gradient_checkpointing)
grep -q -- '--sdpa' <<<"$HELP_TEXT" && EXTRA_ARGS+=(--sdpa)
grep -q -- '--xformers' <<<"$HELP_TEXT" && EXTRA_ARGS+=(--xformers)

# NOTE: sd-scripts expects captions as .txt next to images if you use captioning.
# For DreamBooth method, repeats are encoded in the dataset subfolder name: <repeats>_<token>.
# (make_lora.sh creates that folder automatically.)
accelerate launch --num_processes 1 --mixed_precision bf16 "$TRAIN_SCRIPT" \
  --pretrained_model_name_or_path="$BASE_MODEL" \
  --train_data_dir="$TRAIN_DATA_DIR" \
  --output_dir="$OUTPUT_DIR" \
  --output_name="$OUTPUT_NAME" \
  --resolution="$RESOLUTION" \
  --max_train_steps="$MAX_TRAIN_STEPS" \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --learning_rate=1e-4 \
  --lr_scheduler=cosine \
  --lr_warmup_steps=0 \
  --network_module=networks.lora \
  --network_dim="$NETWORK_DIM" \
  --network_alpha="$NETWORK_ALPHA" \
  --mixed_precision=bf16 \
  --save_precision=bf16 \
  --save_model_as=safetensors \
  --prior_loss_weight=1.0 \
  $CLIP_SKIP_ARG \
  --min_snr_gamma=5 \
  --cache_latents \
  --cache_latents_to_disk \
  --enable_bucket \
  --bucket_no_upscale \
  "${EXTRA_ARGS[@]}" \
  --caption_extension=.txt
```

<a id="SECURITY.md"></a>
### 28. `SECURITY.md`
- Size: 1165 bytes | LOC: 38 | SLOC: 24 | TODOs: 0 | Modified: 2026-02-04 20:01:16 | SHA1: 58f5428d9f97

#### Brief
# Security Policy

#### Auto Summary
Security Policy

#### Content (verbatim)

```markdown
# Security Policy

## Supported Versions

The following table shows which versions of `sdxl_container` are currently being supported with security updates.

| Version | Supported          |
|---------|--------------------|
| main    | :white_check_mark: |

We only provide security updates and fixes for the latest code on the **main** branch.

---

## Reporting a Vulnerability

If you discover a security vulnerability within this project, please help us keep the community safe by following these steps:

- Provide as much detail as possible:
   - A clear description of the vulnerability
   - Steps to reproduce the issue
   - The potential impact
   - Any suggested fixes or mitigations

---

## Security Best Practices for Users

- Always pull the latest image or rebuild the environment to ensure patched dependencies.
- Avoid exposing ports publicly unless necessary.
- Use strong passwords and secrets when connecting to external resources.
- Regularly update Python packages and system dependencies.

---

## Acknowledgements

We deeply appreciate the efforts of security researchers and contributors who help us improve the security of `sdxl_container`.
```

<a id="service-Dockerfile"></a>
### 29. `service/Dockerfile`
- Size: 2057 bytes | LOC: 49 | SLOC: 33 | TODOs: 0 | Modified: 2026-02-04 19:45:04 | SHA1: 68ce54d45420

#### Brief
syntax=docker/dockerfile:1

#### Auto Summary
# syntax=docker/dockerfile:1

#### Content

```dockerfile
# syntax=docker/dockerfile:1
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ARG DEBIAN_FRONTEND=noninteractive
ARG TORCH_INDEX_URL="https://download.pytorch.org/whl/cu121"
ARG SD_SCRIPTS_REPO="https://github.com/kohya-ss/sd-scripts.git"
ARG SD_SCRIPTS_REF="main"
ARG KOHYA_REPO="https://github.com/kohya-ss/kohya_ss.git"
ARG KOHYA_REF="master"

RUN apt-get update && apt-get install -y --no-install-recommends \
    git ca-certificates curl wget \
    python3 python3-pip python3-venv \
    libgl1 libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

RUN python3 -m pip install --upgrade pip setuptools wheel

# PyTorch (CUDA)
RUN python3 -m pip install --extra-index-url ${TORCH_INDEX_URL} 
RUN python3 -m pip install --no-cache-dir -r requirements.txt

# Clone + install training toolchain (with retry to avoid transient network errors / exit 128)
RUN set -eux; \
    retry() { n=0; until [ "$n" -ge 5 ]; do "$@" && break; n=$((n+1)); sleep $((2*n)); done; }; \
    retry git clone --depth 1 --branch "${SD_SCRIPTS_REF}" "${SD_SCRIPTS_REPO}" /opt/sd-scripts; \
    python3 -m pip install -e /opt/sd-scripts; \
    if [ -f /opt/sd-scripts/requirements.txt ]; then \
      (cd /opt/sd-scripts && python3 -m pip install --no-cache-dir -r requirements.txt); \
    fi; \
    retry git clone --depth 1 --branch "${KOHYA_REF}" "${KOHYA_REPO}" /opt/kohya_ss; \
    if [ -f /opt/kohya_ss/requirements.txt ]; then \
      sed -i '/\.\/sd-scripts/d' /opt/kohya_ss/requirements.txt || true; \
      python3 -m pip install --no-cache-dir -r /opt/kohya_ss/requirements.txt; \
    fi; \
    python3 -m pip install --no-cache-dir accelerate transformers sentencepiece pillow diffusers safetensors huggingface_hub peft


ENV HF_HOME=/workspace/cache/hf \
    PIP_CACHE_DIR=/workspace/cache/pip \
    PYTHONUNBUFFERED=1

WORKDIR /workspace

# Keep scripts in the image as a fallback. (Bind-mounts in compose will override them.)
# COPY scripts/ /scripts/
# RUN chmod -R a+rX /scripts && chmod +x /scripts/*.sh || true

ENTRYPOINT ["/bin/bash", "/scripts/entrypoint.sh"]
```

<a id="service-Dockerfile.test"></a>
### 30. `service/Dockerfile.test`
- Size: 2046 bytes | LOC: 49 | SLOC: 39 | TODOs: 0 | Modified: 2026-02-04 19:44:56 | SHA1: 3530727d74f9

#### Brief
# syntax=docker/dockerfile:1
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

#### Auto Summary
# syntax=docker/dockerfile:1

#### Content

```
# syntax=docker/dockerfile:1
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ARG DEBIAN_FRONTEND=noninteractive
ARG TORCH_INDEX_URL="https://download.pytorch.org/whl/cu121"
ARG SD_SCRIPTS_REPO="https://github.com/kohya-ss/sd-scripts.git"
ARG SD_SCRIPTS_REF="main"
ARG KOHYA_REPO="https://github.com/kohya-ss/kohya_ss.git"
ARG KOHYA_REF="master"

RUN apt-get update && apt-get install -y --no-install-recommends \
    git ca-certificates curl wget \
    python3 python3-pip python3-venv \
    libgl1 libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

RUN python3 -m pip install --upgrade pip setuptools wheel

# PyTorch (CUDA)
RUN python3 -m pip install --extra-index-url ${TORCH_INDEX_URL} \
    torch torchvision torchaudio xformers==0.0.34

# Clone + install training toolchain (with retry to avoid transient network errors / exit 128)
RUN set -eux; \
    retry() { n=0; until [ "$n" -ge 5 ]; do "$@" && break; n=$((n+1)); sleep $((2*n)); done; }; \
    retry git clone --depth 1 --branch "${SD_SCRIPTS_REF}" "${SD_SCRIPTS_REPO}" /opt/sd-scripts; \
    python3 -m pip install -e /opt/sd-scripts; \
    if [ -f /opt/sd-scripts/requirements.txt ]; then \
      (cd /opt/sd-scripts && python3 -m pip install --no-cache-dir -r requirements.txt); \
    fi; \
    retry git clone --depth 1 --branch "${KOHYA_REF}" "${KOHYA_REPO}" /opt/kohya_ss; \
    if [ -f /opt/kohya_ss/requirements.txt ]; then \
      sed -i '/\.\/sd-scripts/d' /opt/kohya_ss/requirements.txt || true; \
      python3 -m pip install --no-cache-dir -r /opt/kohya_ss/requirements.txt; \
    fi; \
    python3 -m pip install --no-cache-dir accelerate transformers sentencepiece pillow diffusers safetensors huggingface_hub peft


ENV HF_HOME=/workspace/cache/hf \
    PIP_CACHE_DIR=/workspace/cache/pip \
    PYTHONUNBUFFERED=1

WORKDIR /workspace

# Keep scripts in the image as a fallback. (Bind-mounts in compose will override them.)
# COPY scripts/ /scripts/
# RUN chmod -R a+rX /scripts && chmod +x /scripts/*.sh || true

ENTRYPOINT ["/bin/bash", "/scripts/entrypoint.sh"]
```

<a id="service-pyproject.toml"></a>
### 31. `service/pyproject.toml`
- Size: 421 bytes | LOC: 28 | SLOC: 24 | TODOs: 0 | Modified: 2026-02-04 20:01:44 | SHA1: ca8b390423ab

#### Brief
[tool.ruff]
line-length = 100

#### Auto Summary
[tool.ruff]

#### Content

```toml
[tool.ruff]
line-length = 100
target-version = "py312"
exclude = [
  ".git",
  ".venv",
  "venv",
  "env",
  "__pycache__",
  "build",
  "dist"
]

[tool.ruff.lint]
select = ["E", "F", "I"]
ignore = []

[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "sdxl_container"
version = "0.0.0"
requires-python = ">=3.10"

[tool.setuptools]
packages = ["app", "data"]
```

<a id="service-pytest.ini"></a>
### 32. `service/pytest.ini`
- Size: 258 bytes | LOC: 11 | SLOC: 11 | TODOs: 0 | Modified: 2026-02-04 20:07:39 | SHA1: aee069793309

#### Brief
[pytest]
testpaths = /tests

#### Auto Summary
[pytest]

#### Content

```ini
[pytest]
testpaths = /tests
addopts =
    -q
    --cov=/scripts
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=xml:/reports/ci/coverage.xml
    --cov-report=html:/reports/ci/html
    --cov-fail-under=80
python_files = test_*.py
```

<a id="service-requirements.test.txt"></a>
### 33. `service/requirements.test.txt`
- Size: 93 bytes | LOC: 5 | SLOC: 5 | TODOs: 0 | Modified: 2026-02-04 19:25:24 | SHA1: d0971c380822

#### Brief
# write required libraries.
-r requirements.txt

#### Auto Summary
# write required libraries.

#### Content

```
# write required libraries.
-r requirements.txt
pytest==8.4.2
pytest-cov==7.0.0
ruff==0.14.8
```

<a id="service-requirements.txt"></a>
### 34. `service/requirements.txt`
- Size: 98 bytes | LOC: 5 | SLOC: 5 | TODOs: 0 | Modified: 2026-02-04 19:42:33 | SHA1: 090833a388ad

#### Brief
# write required libraries.
torch==2.10.0

#### Auto Summary
# write required libraries.

#### Content

```
# write required libraries.
torch==2.10.0
torchvision==0.25.0
torchaudio==2.10.0
xformers==0.0.34
```

<a id="tests-.gitkeep"></a>
### 35. `tests/.gitkeep`
- Size: 0 bytes | LOC: 0 | SLOC: 0 | TODOs: 0 | Modified: 2026-02-04 19:26:56 | SHA1: 
#### Content

```
```

<a id="workspace-.gitkeep"></a>
### 36. `workspace/.gitkeep`
- Size: 0 bytes | LOC: 0 | SLOC: 0 | TODOs: 0 | Modified: 2026-02-04 19:00:51 | SHA1: 
#### Content

```
```
